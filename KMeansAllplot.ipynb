{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tslearn.clustering import TimeSeriesKMeans\n",
    "# from tslearn.clustering import silhouette_score\n",
    "\n",
    "from collections import Counter\n",
    "import copy\n",
    "import csv\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "matplotlib.rcParams['font.family']=\"Arial\"\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import peakutils\n",
    "from scipy.spatial.distance import cdist\n",
    "import scipy.stats as stats\n",
    "from skimage import exposure\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn.decomposition #for PCA\n",
    "from sklearn.metrics import silhouette_samples#, silhouette_score\n",
    "import sklearn.preprocessing\n",
    "from tifffile import imread\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import colors\n",
    "# from skimage.external import tifffile as tif\n",
    "from tifffile import imread\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy.stats import wilcoxon\n",
    "from scipy.stats import ranksums\n",
    "\n",
    "\n",
    "# pca = sklearn.decomposition.PCA(n_components=2)\n",
    "\n",
    "# perform_PCA = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dff(df):\n",
    "    trial_f = df.iloc[13:]\n",
    "    trial_dff = (trial_f-trial_f[0:14].mean(axis=0))/trial_f[0:14].mean(axis=0)\n",
    "    return trial_dff\n",
    "\n",
    "def k_resp_splitter4(side_tot, trial_list, resp_trials, noresp_trials):\n",
    "    ind_trial_length = int(side_tot.shape[0]/len(trial_list))\n",
    "    resp_act = []\n",
    "    noresp_act = []\n",
    "    \n",
    "    for i in range(len(trial_list)):\n",
    "        trial = trial_list[i]\n",
    "        if trial in resp_trials:\n",
    "            trial_f = side_tot.iloc[((ind_trial_length*int(trial_list.index(trial)))+13):(ind_trial_length*(int(trial_list.index(trial))+1))].reset_index().drop('index', axis=1)\n",
    "            trial_dff = (trial_f-trial_f[0:14].mean(axis=0))/trial_f[0:14].mean(axis=0)\n",
    "            print('trial_dff shape is', trial_dff.shape)\n",
    "            resp_act.append(trial_dff)\n",
    "\n",
    "        elif trial in noresp_trials:\n",
    "            trial_f = side_tot.iloc[((ind_trial_length*int(trial_list.index(trial)))+13):(ind_trial_length*(int(trial_list.index(trial))+1))].reset_index().drop('index', axis=1)\n",
    "            trial_dff = (trial_f-trial_f[0:14].mean(axis=0))/trial_f[0:14].mean(axis=0)\n",
    "            noresp_act.append(trial_dff)\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "    resp_arr = pd.concat(resp_act, axis=0, ignore_index=True)\n",
    "    print('resp_arr.shape is ', resp_arr.shape)\n",
    "    if noresp_trials == '0':\n",
    "        noresp_arr = resp_arr[:ind_trial_length]\n",
    "        print('noresp_arr.shape is ', noresp_arr.shape)\n",
    "        \n",
    "    else:\n",
    "        noresp_arr = pd.concat(noresp_act, axis=0, ignore_index=True)\n",
    "    return resp_arr, noresp_arr\n",
    "\n",
    "def fiji_elbow4(fijiarr):\n",
    "    X = fijiarr\n",
    "    \n",
    "    distortions = []\n",
    "    K = range(1,10)\n",
    "    for k in K:\n",
    "        kmeanModel = KMeans(n_clusters=k).fit(X)\n",
    "        kmeanModel.fit(X)\n",
    "        distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / len(X))\n",
    "\n",
    "    # Plot the elbow\n",
    "#     ax=plt.subplot(111)\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_dpi(300)\n",
    "#     fig = plt.figure(figsize=(30, 25), dpi=300)\n",
    "    plt.rc('xtick', labelsize=15)\n",
    "    plt.rc('ytick', labelsize=15)\n",
    "    \n",
    "#     xticks = [0.24,0.29, 0.34]\n",
    "#     xlabels = ['0.25','0.30', '0.35']\n",
    "# #     xticks = [0.4,0.6, 0.8, 1.0]\n",
    "# #     xlabels = ['0.4','0.6', '0.8', '1.0']\n",
    "#     ax.set_yticks(xticks)\n",
    "#     ax.set_yticklabels(xlabels)\n",
    "    \n",
    "    plt.plot(K, distortions, 'bx-')\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.title('The Elbow Method showing the optimal k', pad=10)\n",
    "#     plt.ylim(0.1,1.4)\n",
    "    plt.savefig('complete_elbow.png')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    n_clusters = int(input(\"How many clusters for kMeans: \"))\n",
    "    \n",
    "    return n_clusters\n",
    "\n",
    "def fiji_kmeans4(n_clusters, fijiarr):\n",
    "    \n",
    "    n_clusters = n_clusters\n",
    "    fijiarr = fijiarr\n",
    "    \n",
    "    kmeans = sklearn.cluster.KMeans(n_clusters)\n",
    "    clustered_data = kmeans.fit(fijiarr)\n",
    "\n",
    "    batch_labelled_total_list = kmeans.labels_\n",
    "    \n",
    "    return batch_labelled_total_list\n",
    "\n",
    "def fiji_kmeansA(n_clusters, fijiarr):\n",
    "    \n",
    "    n_clusters = n_clusters\n",
    "    fijiarr = fijiarr\n",
    "    \n",
    "    kmeans = sklearn.cluster.KMeans(n_clusters)\n",
    "    clustered_data = kmeans.fit(fijiarr)\n",
    "\n",
    "    batch_labelled_total_list = kmeans.labels_\n",
    "    \n",
    "    return batch_labelled_total_list, kmeans\n",
    "\n",
    "def fiji_organize_roisA(batch_labelled_total_list, fijiarr):\n",
    "    '''Used to split ROIs back into their original planes AND \n",
    "    into the new clusters'''\n",
    "    labels_by_plane = []\n",
    "    clustered_temporal_footprints = {}\n",
    "    label_proportions = {}\n",
    "    \n",
    "    side_dff = fijiarr\n",
    "#     print('fiji_organize_roisA',clustered_temporal_footprints.keys())\n",
    "    \n",
    "    for label, calcium_trace in zip(batch_labelled_total_list, side_dff):\n",
    "#         calcium_trace = k_concat_splitterA(concat_trace, trials)\n",
    "#         k_concat_splitter(side_tot, trial_list)\n",
    "        if label not in clustered_temporal_footprints.keys():\n",
    "            clustered_temporal_footprints[label] = []\n",
    "            clustered_temporal_footprints[label].append(calcium_trace)\n",
    "        else:\n",
    "            clustered_temporal_footprints[label].append(calcium_trace)\n",
    "    for cluster in clustered_temporal_footprints.keys():\n",
    "        label_proportions[cluster] = int(round(10*(len(clustered_temporal_footprints[cluster])/len(batch_labelled_total_list))))\n",
    "       \n",
    "    if 0 in label_proportions.values():\n",
    "        for i in label_proportions:\n",
    "            if label_proportions[i] == 0:\n",
    "                label_proportions[i] = 1\n",
    "    return clustered_temporal_footprints, label_proportions\n",
    "\n",
    "def k_concat_splitterA(side_tot, trial_list):\n",
    "    ind_trial_length = int(side_tot.shape[0]/len(trial_list))\n",
    "    print('ind_trial_length is', ind_trial_length)\n",
    "    split_act = []\n",
    "\n",
    "    for i in range(len(trial_list)):\n",
    "        trial = trial_list[i]\n",
    "        if trial in trial_list:\n",
    "            split_act.append(side_tot[(ind_trial_length*int(trial_list.index(trial))):(ind_trial_length*(int(trial_list.index(trial))+1))].reshape(-1,1))#.reset_index().drop('index', axis=1))\n",
    "\n",
    "    split_act = np.concatenate(split_act, axis=1)#, ignore_index=True)\n",
    "    split_mean = np.mean(split_act, axis=1)#I think this is a very elegant solution, but causes problems down the line\n",
    "    return split_act#~\n",
    "\n",
    "\n",
    "def plot_cluster_avgsONLYA(clustered_temporal_footprints, plot_frames, plot_duration, name_for_file, n_clusters):\n",
    "    '''Plots a line graph of the average calcium activity of each cluster.'''\n",
    "    trial_duration = plot_duration\n",
    "    total_frames = plot_frames\n",
    "    avg_file_csv = str(name_for_file)+'.csv'\n",
    "    avg_file_plot_png = str(name_for_file)+'.png'\n",
    "    trace_counter = 0\n",
    "    mean_dict = {}\n",
    "    \n",
    "    for i in range(0,n_clusters):\n",
    "#         print(type(clustered_temporal_footprints[i]))\n",
    "        print(len(clustered_temporal_footprints[i]))\n",
    "        split_act = np.asarray(clustered_temporal_footprints[i]).T\n",
    "#         split_act = np.concatenate(clustered_temporal_footprints[0], axis=1)#, ignore_index=True)#~\n",
    "#         split_act = clustered_temporal_footprints[i]\n",
    "        \n",
    "        mean_dict[i] = np.mean(split_act, axis=1)#~\n",
    "        print('mean_dict[',i,'].shape is ', mean_dict[i].shape)\n",
    "\n",
    "        ######mean_dict[i] = np.mean(clustered_temporal_footprints[i], axis=0)\n",
    "        \n",
    "    #saves csv of mean cluster activity\n",
    "    cluster_averages = pd.DataFrame(mean_dict).T\n",
    "#     print('The shape of ')\n",
    "    cluster_averages.to_csv((\"cluster_avg_traces\"+avg_file_csv))\n",
    "    \n",
    "    \n",
    "    colors = ['tomato', 'seagreen', 'deepskyblue', 'orange', 'skyblue', 'magenta']\n",
    "    xVals = np.linspace(0, trial_duration, total_frames)\n",
    "    legend_names = mean_dict.keys()\n",
    "    \n",
    "    plt.rc('xtick', labelsize=15)\n",
    "    plt.rc('ytick', labelsize=15)\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_dpi(300)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    xticks = [-0.2,0, 0.2, 0.4, 0.6]\n",
    "    xlabels = ['-0.2','0', '0.2', '0.4', '0.6']\n",
    "#     xticks = [-0.15,0, 0.15, 0.3]\n",
    "#     xlabels = ['-0.15','0', '0.15', '0.3']    \n",
    "    \n",
    "    ax.set_yticks(xticks)\n",
    "    ax.set_yticklabels(xlabels, fontname=\"Arial\")\n",
    "    plt.title(name_for_file, pad=15)    \n",
    "    plt.title(name_for_file, pad=15)\n",
    "    plt.ylim(-0.2,0.7)\n",
    "#     plt.ylim(-0.1,0.3)\n",
    "    \n",
    "    for i in (mean_dict.keys()):\n",
    "\n",
    "\n",
    "        plt.plot(xVals, mean_dict[i], color=colors[i])\n",
    "\n",
    "\n",
    "#         plt.ylim(-0.2,0.4)\n",
    "\n",
    "    plt.savefig((\"cluster_avg_traces_\"+avg_file_plot_png))\n",
    "    plt.close()\n",
    "    return mean_dict\n",
    "\n",
    "def fiji_plot_heatmap_clustersA(label_proportions, clustered_temporal_footprints, n_clusters, trial_duration, name_for_file, frames_per_trial):\n",
    "    '''Plots heatmap raster plots of different clusters.'''\n",
    "#     Things to fix:\n",
    "    subplot_height = sum(label_proportions.values())\n",
    "    heat_file_plot_png = str(name_for_file)+'.png'\n",
    "\n",
    "    subplot_start = 0\n",
    "    fig = plt.figure(figsize=(30, 25), dpi=300)\n",
    "    gs0 = gridspec.GridSpec(1,1, figure=fig)\n",
    "    gs00 = gridspec.GridSpecFromSubplotSpec(subplot_height, 11, subplot_spec=gs0[0], wspace=0.0)\n",
    "#     extend = [0,40.2,0,110] # sets size of the plots, not sure if I still use this. Will check after I solve the colour plotting issue\n",
    "    split2 = []\n",
    "    for i in range(n_clusters):\n",
    "        if i in clustered_temporal_footprints.keys():\n",
    "#             split_act1 = np.concatenate(clustered_temporal_footprints[i], axis=1)#~\n",
    "            split_act1 = np.asarray(clustered_temporal_footprints[i]).T\n",
    "            print('clustered_temporal_footprints.keys', str(i))\n",
    "            print('split_act1 is', split_act1.shape)\n",
    "        else:\n",
    "            split_act1 = np.ones((frames_per_trial,1), np.int8)\n",
    "        #!!#!##!##!!#!###!#!#!#!#!#!#!#\n",
    "        \n",
    "        \n",
    "        split2.append(split_act1)\n",
    "#         fig.add_subplot(gs00[(subplot_start):(subplot_start+label_proportions[i]),0:-2])\n",
    "    split_act2 = np.concatenate(split2, axis=1)\n",
    "#     norm = colors.Normalize()#vmin=split_act2.T.min(), vmax=split_act2.T.max())\n",
    "    norm = colors.Normalize(vmin=split_act2.T.min(), vmax=0.8)\n",
    "    plt.imshow(split_act2.T, cmap = 'viridis', aspect = 'auto', extent= [0, trial_duration, 0, split_act2.shape[1]], norm=norm)\n",
    "#     plt.colorbar()\n",
    "#         plt.imshow(split_act1.T, cmap = 'plasma', aspect = 'auto', extent= [0, trial_duration, 0, split_act1.shape[1]], norm=norm)\n",
    "    ####this is the old stuff I can go back to\n",
    "#         split_act1 = np.concatenate(clustered_temporal_footprints[i], axis=1)#~\n",
    "#         fig.add_subplot(gs00[(subplot_start):(subplot_start+label_proportions[i]),0:-2])\n",
    "#         plt.ylabel(('Cluster '+str(i)), fontsize=25)\n",
    "# #         norm = colors.Normalize(vmin=0, vmax=1)\n",
    "#         plt.imshow(split_act1.T, cmap = 'plasma', aspect = 'auto', extent= [0, trial_duration, 0, split_act1.shape[1]])\n",
    "# #         plt.imshow(split_act1.T, cmap = 'plasma', aspect = 'auto', extent= [0, trial_duration, 0, split_act1.shape[1]], norm=norm)\n",
    "\n",
    "        #subplot_start += label_proportions[i]!!!This gets kept and uncommented\n",
    "#     fig.suptitle(str(video1_name[:-10]), fontsize=40)#\"{}\".format(video_name[:-4]), fontsize=40)\n",
    "#     plt.show()\n",
    "    plt.savefig(('vHeat_ClustersCleanNORMnosep'+heat_file_plot_png))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_clust_indandavg(k, clustered_temporal_footprints, name):\n",
    "    #plots all trials in a cluster, comment back in to generate plots\n",
    "    plot_name = name\n",
    "#     for i in range(0,k):\n",
    "# #         clust = np.concatenate(clustered_temporal_footprints[i], axis=1)\n",
    "#         clust = np.asarray(clustered_temporal_footprints[i]).T\n",
    "#         df = pd.DataFrame(clust)\n",
    "#         dfmean = df.mean(axis=1)\n",
    "#         dfstd = df.std(axis=1)\n",
    "#         ax=plt.subplot(111)\n",
    "#         plt.plot(df, c='silver')\n",
    "#         plt.plot(dfmean, c='black')\n",
    "#         ax.spines['right'].set_visible(False)\n",
    "#         ax.spines['top'].set_visible(False)        \n",
    "# #         plt.ylim(-1,3.5)#!#!\n",
    "#         plt.ylim(-0.2,0.7)\n",
    "#         plt.savefig(plot_name+str(i)+'ing&avg.png')\n",
    "#         plt.clf()\n",
    "        \n",
    "#         r = range(0,df.shape[0])\n",
    "#         fig, ax = plt.subplots()\n",
    "#         ax.plot(dfmean)\n",
    "#         ax.fill_between(r, dfmean+dfstd, dfmean-dfstd, alpha=0.2)\n",
    "#         ax.spines['right'].set_visible(False)\n",
    "#         ax.spines['top'].set_visible(False)  \n",
    "#         plt.ylim(-0.2,0.7)#!#!\n",
    "# #         plt.ylim(-1,3.5)#!#!\n",
    "#         plt.savefig(plot_name+str(i)+'std.png')\n",
    "#         plt.clf()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_concated_clusters(batch_side_total_list, side_dff, file_stem, side, n_clusters):\n",
    "    side_arr = np.array(side_dff).T\n",
    "    clustered_tot_footprints = {}\n",
    "    label_proportions = {}\n",
    "    for label, concat_trace in zip(batch_side_total_list, side_arr):\n",
    "        if label not in clustered_tot_footprints.keys():\n",
    "            clustered_tot_footprints[label] = []\n",
    "            clustered_tot_footprints[label].append(concat_trace.reshape(-1,1))\n",
    "        else:\n",
    "            clustered_tot_footprints[label].append(concat_trace.reshape(-1,1))\n",
    "            \n",
    "#     return clustered_tot_footprints\n",
    "    for cluster in clustered_tot_footprints.keys():\n",
    "        label_proportions[cluster] = int(round(10*(len(clustered_tot_footprints[cluster])/len(batch_side_total_list))))\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        if i not in label_proportions.keys():\n",
    "            label_proportions.update({i:1})\n",
    "        \n",
    "    print('label proportions are:')\n",
    "    print(label_proportions.values())\n",
    "    print(label_proportions.keys())\n",
    "    if 0 in label_proportions.values():\n",
    "        for i in label_proportions:\n",
    "            if label_proportions[i] == 0:\n",
    "                label_proportions[i] = 1\n",
    "                \n",
    "    total_frames = int(side_dff.shape[0])\n",
    "    trial_duration = total_frames/2.73\n",
    "    name_for_file = file_stem+side+'concatPlot'\n",
    "#     plot_cluster_avgsONLY4(clustered_tot_footprints, plot_frames, plot_duration, name_for_file)\n",
    "    \n",
    "    avg_file_csv = str(name_for_file)+'.csv'\n",
    "    avg_file_plot_png = str(name_for_file)+'.png'\n",
    "    trace_counter = 0\n",
    "    concat_mean_dict = {}\n",
    "    jk = 0\n",
    "\n",
    "    for i in clustered_tot_footprints:\n",
    "        if i in clustered_tot_footprints.keys():\n",
    "            split_act = np.concatenate(clustered_tot_footprints[i], axis=1)#, ignore_index=True)#~\n",
    "        else:\n",
    "            split_act = np.ones((side_dff.shape[0],1), np.int8)\n",
    "        concat_mean_dict[i] = np.mean(split_act, axis=1)#~\n",
    "        if jk == 0:\n",
    "            jk+=1\n",
    "#         ######mean_dict[i] = np.mean(clustered_temporal_footprints[i], axis=0)\n",
    "        \n",
    "#     saves csv of mean cluster activity\n",
    "    cluster_averages = pd.DataFrame(concat_mean_dict).T\n",
    "    cluster_averages.to_csv((\"cluster_avg_traces\"+avg_file_csv))\n",
    "    \n",
    "    \n",
    "    colors = ['tomato', 'seagreen', 'deepskyblue', 'orange', 'skyblue', 'magenta']\n",
    "    xVals = np.linspace(0, trial_duration, total_frames)\n",
    "    legend_names = concat_mean_dict.keys()\n",
    "    \n",
    "    \n",
    "    plt.rc('xtick', labelsize=15)\n",
    "    plt.rc('ytick', labelsize=15)\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_dpi(300)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    xticks = [-0.2,0, 0.2, 0.4, 0.6]\n",
    "    xlabels = ['-0.2','0', '0.2', '0.4', '0.6']\n",
    "#     xticks = [-0.15,0, 0.15, 0.3]\n",
    "#     xlabels = ['-0.15','0', '0.15', '0.3']     \n",
    "    \n",
    "    ax.set_yticks(xticks)\n",
    "    ax.set_yticklabels(xlabels, fontname=\"Arial\")\n",
    "    plt.title(name_for_file, pad=15)    \n",
    "    plt.ylim(-0.2,0.7)\n",
    "#     plt.ylim(-0.1,0.3)\n",
    "    \n",
    "  \n",
    "    \n",
    "    for i in (concat_mean_dict.keys()):\n",
    "#         plt.rc('xtick', labelsize=15)\n",
    "#         plt.rc('ytick', labelsize=15)\n",
    "#         fig, ax = plt.subplots()\n",
    "#         fig.set_dpi(300)\n",
    "#         ax.spines['right'].set_visible(False)\n",
    "#         ax.spines['top'].set_visible(False)\n",
    "#         xticks = [-0.2,0, 0.2, 0.4, 0.6]\n",
    "#         xlabels = ['-0.2','0', '0.2', '0.4', '0.6']\n",
    "#         ax.set_yticks(xticks)\n",
    "#         ax.set_yticklabels(xlabels)\n",
    "#         plt.title(name_for_file, pad=10)\n",
    "        plt.plot(xVals, concat_mean_dict[i], color=colors[i])\n",
    "        \n",
    "#         plt.ylim(-0.2,0.7)\n",
    "#         plt.ylim(-0.2,0.4)\n",
    "\n",
    "    plt.savefig((\"cluster_avg_traces_\"+avg_file_plot_png))\n",
    "    plt.close()\n",
    "\n",
    "    subplot_height = sum(label_proportions.values())\n",
    "    heat_file_plot_png = str(name_for_file)+'.png'\n",
    "\n",
    "    subplot_start = 0\n",
    "    fig = plt.figure(figsize=(30, 25), dpi=300)\n",
    "    gs0 = gridspec.GridSpec(1,1, figure=fig)\n",
    "    gs00 = gridspec.GridSpecFromSubplotSpec(subplot_height, 11, subplot_spec=gs0[0], wspace=0.0)\n",
    "#     extend = [0,40.2,0,110] # sets size of the plots, not sure if I still use this. Will check after I solve the colour plotting issue\n",
    "    for i in range(n_clusters):\n",
    "        if i in clustered_tot_footprints.keys():\n",
    "            split_act1 = np.concatenate(clustered_tot_footprints[i], axis=1)#, ignore_index=True)#~\n",
    "        else:\n",
    "            split_act1 = np.ones((side_dff.shape[0],1), np.int8)\n",
    "#         split_act1 = np.concatenate(clustered_tot_footprints[i], axis=1)#~\n",
    "        fig.add_subplot(gs00[(subplot_start):(subplot_start+label_proportions[i]),0:-2])\n",
    "        plt.ylabel(('Cluster '+str(i)), fontsize=25)\n",
    "        plt.imshow(split_act1.T, cmap = 'seismic', aspect = 'auto', extent= [0, trial_duration, 0, split_act1.shape[1]])\n",
    "#         plt.colorbar()\n",
    "#         plt.imshow(clustered_temporal_footprints[i], cmap = 'plasma', aspect = 'auto', extent= [0, trial_duration, 0, len(clustered_temporal_footprints[i])])        \n",
    "        subplot_start += label_proportions[i]\n",
    "#     fig.suptitle(str(video1_name[:-10]), fontsize=40)#\"{}\".format(video_name[:-4]), fontsize=40)\n",
    "#     plt.show()\n",
    "    plt.savefig(('vHeat_ClustersClean'+heat_file_plot_png))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_avgsONLYA(clustered_temporal_footprints, plot_frames, plot_duration, name_for_file, n_clusters):\n",
    "    '''Plots a line graph of the average calcium activity of each cluster.'''\n",
    "    trial_duration = plot_duration\n",
    "    total_frames = plot_frames\n",
    "    avg_file_csv = str(name_for_file)+'.csv'\n",
    "    avg_file_plot_png = str(name_for_file)+'.png'\n",
    "    trace_counter = 0\n",
    "    mean_dict = {}\n",
    "    sem_dict = {}\n",
    "\n",
    "    for i in range(0,n_clusters):\n",
    "        print(len(clustered_temporal_footprints[i]))\n",
    "        split_act = np.asarray(clustered_temporal_footprints[i]).T\n",
    "        \n",
    "        mean_dict[i] = np.mean(split_act, axis=1)#~\n",
    "        print(split_act.shape[1])\n",
    "#         b=input('aw shit here we go again')\n",
    "        sem_dict[i] = np.std(split_act, axis=1)#/math.sqrt(split_act.shape[1])\n",
    "\n",
    "        ######mean_dict[i] = np.mean(clustered_temporal_footprints[i], axis=0)\n",
    "        \n",
    "    #saves csv of mean cluster activity\n",
    "    cluster_averages = pd.DataFrame(mean_dict)#.T\n",
    "#     print('The shape of ')\n",
    "    cluster_averages.to_csv((\"cluster_avg_traces\"+avg_file_csv))\n",
    "    \n",
    "    \n",
    "    colors = ['tomato', 'seagreen', 'deepskyblue', 'orange', 'skyblue', 'magenta']\n",
    "    xVals = np.linspace(0, trial_duration, total_frames)\n",
    "    legend_names = mean_dict.keys()\n",
    "    \n",
    "    plt.rc('xtick', labelsize=15)\n",
    "    plt.rc('ytick', labelsize=15)\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_dpi(300)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    xticks = [-0.2,0, 0.2, 0.4, 0.6]\n",
    "    xlabels = ['-0.2','0', '0.2', '0.4', '0.6']  \n",
    "#     xticks = [-0.15,0, 0.15, 0.3]\n",
    "#     xlabels = ['-0.15','0', '0.15', '0.3'] \n",
    "    ax.set_yticks(xticks)\n",
    "    ax.set_yticklabels(xlabels, fontname=\"Arial\")\n",
    "    plt.title(name_for_file, pad=15)    \n",
    "    plt.ylim(-0.2,0.7)\n",
    "#     plt.ylim(-0.1,0.3)\n",
    "\n",
    "    \n",
    "\n",
    "    for i in (mean_dict.keys()):\n",
    "#         plt.rc('xtick', labelsize=15)\n",
    "#         plt.rc('ytick', labelsize=15)\n",
    "#         fig, ax = plt.subplots()\n",
    "#         fig.set_dpi(300)\n",
    "        \n",
    "#         xticks = [-0.2,0, 0.2, 0.4, 0.6]\n",
    "#         xlabels = ['-0.2','0', '0.2', '0.4', '0.6']\n",
    "#         ax.set_yticks(xticks)\n",
    "#         ax.set_yticklabels(xlabels)\n",
    "#         plt.title(name_for_file, pad=10)\n",
    "        \n",
    "        dfstd = mean_dict[i].std()#/math.sqrt(len(mean_dict[i]))\n",
    "\n",
    "#         print(mean_dict[i].shape)\n",
    "# #         b = input('here we go')\n",
    "        ax.plot(xVals, mean_dict[i], color=colors[i])\n",
    "        ax.fill_between(xVals, mean_dict[i]+sem_dict[i], mean_dict[i]-sem_dict[i], alpha=0.2, color=colors[i],edgecolor='None')\n",
    "#         ax.spines['right'].set_visible(False)\n",
    "#         ax.spines['top'].set_visible(False) \n",
    "#         plt.title(name_for_file, pad=10)\n",
    "#         plt.ylim(-0.25,0.7)\n",
    "# #         plt.ylim(-0.2,0.4)\n",
    "\n",
    "    plt.savefig((\"cluster_avg_traces_\"+avg_file_plot_png))\n",
    "    plt.close()\n",
    "    return mean_dict\n",
    "\n",
    "def remove_outliers(raw_df):\n",
    "    pdstd = raw_df.std()\n",
    "    pdmean = raw_df.mean()\n",
    "    pddiff = raw_df-pdmean\n",
    "    \n",
    "    lim = pdstd*3\n",
    "    bound = raw_df.where(abs(pddiff)<lim)\n",
    "    t2=bound.dropna()\n",
    "    return t2\n",
    "\n",
    "def tenind_fun(df):#$%\n",
    "    tenindtot = []\n",
    "    for i in df.columns:\n",
    "        act=pd.Series(df[i])\n",
    "        mval = act.iloc[62:72].max()#$%^\n",
    "        mvalind= (act==mval).idxmax()\n",
    "        revmax =pd.Series(act[:mvalind+1].values[::-1])\n",
    "        frommax = (revmax<(mval*0.1)).idxmax()\n",
    "\n",
    "        tenind = mvalind-frommax\n",
    "        tenindtot.append(tenind)\n",
    "\n",
    "    return tenindtot\n",
    "\n",
    "def percentind_fun(df, percent):#$%\n",
    "\n",
    "    percentindtot = []\n",
    "    for i in df.columns:\n",
    "        act=pd.Series(df[i])\n",
    "        mval = act.iloc[62:72].max()#$%^\n",
    "        mvalind= (act==mval).idxmax()\n",
    "#         (a==56).idxmax()\n",
    "        revmax =pd.Series(act[:mvalind+1].values[::-1])\n",
    "#         revmax =pd.Series(act[:act.idxmax()+1].values[::-1])\n",
    "        frommax = (revmax<(mval*percent)).idxmax()\n",
    "        percentind = mvalind-frommax\n",
    "        percentindtot.append(percentind)\n",
    "\n",
    "    return percentindtot\n",
    "\n",
    "def complete_stats(df, fname):\n",
    "\n",
    "    tot_dff=df\n",
    "    fname=fname\n",
    "    maxval = tot_dff.iloc[55:109,:].max()#new iloc for finding max values\n",
    "    maxval =remove_outliers(maxval)\n",
    "    \n",
    "    maxind = (tot_dff == maxval).idxmax()\n",
    "    \n",
    "    truemaxtime = (maxind+13)/2.73\n",
    "    \n",
    "    baseval = abs(tot_dff.iloc[0:14,:].mean())\n",
    "    endval = abs(tot_dff.iloc[-6:,:].mean())\n",
    "    tenbaseval = baseval*1.1\n",
    "    ninetymaxval = maxval*0.9\n",
    "    tenmaxval = maxval*0.2\n",
    "    totdecay = 100-((endval/maxval)*100)\n",
    "    totdecay =remove_outliers(totdecay)\n",
    "    \n",
    "    tenind = pd.Series(percentind_fun(df, 0.1))\n",
    "    ninetyind = pd.Series(percentind_fun(df, 0.9))\n",
    "    \n",
    "    risetime = (ninetyind-tenind)/2.73\n",
    "    \n",
    "    finaldf = pd.concat((baseval, maxval, maxind, truemaxtime, tenbaseval, tenmaxval, ninetymaxval, risetime, totdecay, ninetyind, tenind), axis=1)\n",
    "    finaldf.columns = ['baseline', 'maxvalue', 'maxind', 'truemaxtime', '10%base', '10%max', '90%max', 'risetimemax', '%decay', 'ninetyind', 'tenind']\n",
    "    finaldf.to_csv(fname+'stats.csv')\n",
    "    return finaldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_clust(file_stem, group, stain, kmeans, n_clusters, img_speed, left_tot_rois, right_tot_rois, batch_labelled_total_list):\n",
    "    \n",
    "    file_stem_end = '.csv'\n",
    "    left_df = pd.read_csv(file_stem+group+'resp_'+stain+'_IpsAllStim'+file_stem_end, index_col=0)\n",
    "    left_nodf = pd.read_csv(file_stem+group+'noresp_'+stain+'_IpsAllStim'+file_stem_end, index_col=0)\n",
    "    right_df = pd.read_csv(file_stem+group+'resp_'+stain+'_ContAllStim'+file_stem_end, index_col=0)\n",
    "    right_nodf = pd.read_csv(file_stem+group+'noresp_'+stain+'_ContAllStim'+file_stem_end, index_col=0)\n",
    "    left_dff = get_dff(left_df)\n",
    "    left_nodff = get_dff(left_nodf)\n",
    "    right_dff = get_dff(right_df)\n",
    "    right_nodff = get_dff(right_nodf)\n",
    "\n",
    "    #get info for later functions\n",
    "    left_tot_rois =left_tot_rois\n",
    "    right_tot_rois = right_tot_rois\n",
    "    frames_per_trial = int(left_dff.shape[0])\n",
    "    trial_duration = frames_per_trial/2.73\n",
    "    left_rois = left_df.shape[1]\n",
    "    right_rois = right_df.shape[1]\n",
    "\n",
    "    #combine sides\n",
    "    tot_dff = pd.concat([left_dff, right_dff], axis=1)\n",
    "    tot_nodff = pd.concat([left_nodff, right_nodff], axis=1)\n",
    "    fijiarr = np.array(tot_dff).T\n",
    "    fiji_noarr = np.array(tot_nodff).T\n",
    "\n",
    "    batch_orig_list =batch_labelled_total_list\n",
    "    batch_left = list(batch_orig_list[:left_rois])\n",
    "    batch_right = list(batch_orig_list[left_tot_rois:left_tot_rois+right_rois])\n",
    "    batch_labelled_total_list = batch_left+batch_right\n",
    "    print('we are in ', str(group))\n",
    "    print('The left and right dffs are of shape:', left_dff.shape, right_dff.shape)\n",
    "    print('tot_dff and fijiarr are of shape:', tot_dff.shape, fijiarr.shape)\n",
    "    print('batch_orig_list len is', len(batch_orig_list))\n",
    "    print('batch_labelled_total_list len is', len(batch_labelled_total_list))\n",
    "    print('left tot rois and left rois are', left_tot_rois, left_rois)\n",
    "    print('right tot rois and right rois are', right_tot_rois, right_rois)\n",
    "\n",
    "    stats_dict = {}\n",
    "    \n",
    "    for side in ('Ips', 'Cont'):\n",
    "        if side == 'Ips':               \n",
    "            name_for_file = file_stem+group+stain+side+'Resp'\n",
    "            clustered_temporal_footprints, label_proportions = fiji_organize_roisA(batch_labelled_total_list[0:left_rois],  fijiarr[0:left_rois])\n",
    "\n",
    "            plot_frames = frames_per_trial\n",
    "            plot_duration = plot_frames/img_speed\n",
    "            mean_dict= plot_cluster_avgsONLYA(clustered_temporal_footprints, plot_frames, plot_duration, name_for_file, n_clusters)\n",
    "            fiji_plot_heatmap_clustersA(label_proportions, clustered_temporal_footprints, n_clusters, trial_duration, name_for_file, frames_per_trial)\n",
    "            plot_clust_indandavg(n_clusters, clustered_temporal_footprints, name_for_file)\n",
    "            \n",
    "            clustdf = pd.DataFrame(clustered_temporal_footprints[1]).T\n",
    "            print(type(clustdf))\n",
    "            stat_key = side+'Resp'+str(1)\n",
    "            stats_dict[stat_key] = complete_stats(clustdf, stat_key)#$%^\n",
    "                    \n",
    "            for i in range(0,n_clusters):\n",
    "                clustdf = pd.DataFrame(clustered_temporal_footprints[i]).T\n",
    "                stat_key = side+'Resp'+str(i)\n",
    "                stats_dict[stat_key] = complete_stats(clustdf, stat_key)#$%^\n",
    "\n",
    "            name_for_file = file_stem+group+stain+side+'NoResp'\n",
    "            clustered_temporal_footprints, label_proportions = fiji_organize_roisA(batch_labelled_total_list[0:left_rois], fiji_noarr[0:left_rois])\n",
    "            mean_dict = plot_cluster_avgsONLYA(clustered_temporal_footprints, plot_frames, plot_duration, name_for_file, n_clusters)\n",
    "#             mean_dict = plot_cluster_avgsONLY4(clustered_temporal_footprints, plot_frames, plot_duration, name_for_file)\n",
    "            fiji_plot_heatmap_clustersA(label_proportions, clustered_temporal_footprints, n_clusters, trial_duration, name_for_file, frames_per_trial)\n",
    "            plot_clust_indandavg(n_clusters, clustered_temporal_footprints, name_for_file)\n",
    "            for i in range(0,n_clusters):\n",
    "                clustdf = pd.DataFrame(clustered_temporal_footprints[i]).T\n",
    "                stat_key = side+'NoResp'+str(i)\n",
    "                stats_dict[stat_key] = complete_stats(clustdf, stat_key)\n",
    "\n",
    "        elif side == 'Cont':\n",
    "            name_for_file = file_stem+group+stain+side+'Resp'\n",
    "            clustered_temporal_footprints, label_proportions = fiji_organize_roisA(batch_labelled_total_list[left_rois:],  fijiarr[left_rois:])\n",
    "\n",
    "            plot_frames = frames_per_trial\n",
    "            plot_duration = plot_frames/img_speed\n",
    "            mean_dict= plot_cluster_avgsONLYA(clustered_temporal_footprints, plot_frames, plot_duration, name_for_file, n_clusters)\n",
    "            fiji_plot_heatmap_clustersA(label_proportions, clustered_temporal_footprints, n_clusters, trial_duration, name_for_file, frames_per_trial)\n",
    "            plot_clust_indandavg(n_clusters, clustered_temporal_footprints, name_for_file)\n",
    "            for i in range(0,n_clusters):\n",
    "                clustdf = pd.DataFrame(clustered_temporal_footprints[i]).T\n",
    "                stat_key = side+'Resp'+str(i)\n",
    "                stats_dict[stat_key] = complete_stats(clustdf, stat_key)\n",
    "\n",
    "            name_for_file = file_stem+group+stain+side+'NoResp'\n",
    "            clustered_temporal_footprints, label_proportions = fiji_organize_roisA(batch_labelled_total_list[left_rois:], fiji_noarr[left_rois:])\n",
    "            mean_dict = plot_cluster_avgsONLYA(clustered_temporal_footprints, plot_frames, plot_duration, name_for_file, n_clusters)\n",
    "#             mean_dict = plot_cluster_avgsONLY4(clustered_temporal_footprints, plot_frames, plot_duration, name_for_file)\n",
    "            fiji_plot_heatmap_clustersA(label_proportions, clustered_temporal_footprints, n_clusters, trial_duration, name_for_file, frames_per_trial)\n",
    "            plot_clust_indandavg(n_clusters, clustered_temporal_footprints, name_for_file)\n",
    "            for i in range(0,n_clusters):\n",
    "                clustdf = pd.DataFrame(clustered_temporal_footprints[i]).T\n",
    "                stat_key = side+'NoResp'+str(i)\n",
    "                stats_dict[stat_key] = complete_stats(clustdf, stat_key)\n",
    "\n",
    "    name_for_file = file_stem+group+stain+'combined'+'Resp'\n",
    "    plot_combined_sides(batch_labelled_total_list, fijiarr, frames_per_trial, img_speed, name_for_file, n_clusters, trial_duration)\n",
    "    name_for_file = file_stem+group+stain+'combined'+'noResp'\n",
    "    plot_combined_sides(batch_labelled_total_list, fiji_noarr, frames_per_trial, img_speed, name_for_file, n_clusters, trial_duration)\n",
    "    return stats_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_combined_sides(batch_labelled_total_list, fijiarr, frames_per_trial, img_speed, name_for_file, n_clusters, trial_duration):\n",
    "    clustered_temporal_footprints, label_proportions = fiji_organize_roisA(batch_labelled_total_list,  fijiarr)\n",
    "    plot_frames = frames_per_trial\n",
    "    plot_duration = plot_frames/img_speed\n",
    "    mean_dict= plot_cluster_avgsONLYA(clustered_temporal_footprints, plot_frames, plot_duration, name_for_file, n_clusters)\n",
    "    fiji_plot_heatmap_clustersA(label_proportions, clustered_temporal_footprints, n_clusters, trial_duration, name_for_file, frames_per_trial)\n",
    "\n",
    "    \n",
    "def combined_stats(batch_labelled_total_list, left_rois, fijiarr, n_clusters):\n",
    "    stats_dict = {}\n",
    "    for side in ('Ips', 'Cont'):\n",
    "        if side == 'Ips':\n",
    "            clustered_temporal_footprints, label_proportions = fiji_organize_roisA(batch_labelled_total_list[0:left_rois],  fijiarr[0:left_rois])\n",
    "            for i in range(0,n_clusters):\n",
    "                clustdf = pd.DataFrame(clustered_temporal_footprints[i]).T\n",
    "                stat_key = side+'Resp'+str(i)\n",
    "                stats_dict[stat_key] = complete_stats(clustdf, stat_key)\n",
    "                \n",
    "        elif side == 'Cont':\n",
    "            clustered_temporal_footprints, label_proportions = fiji_organize_roisA(batch_labelled_total_list[left_rois:],  fijiarr[left_rois:])\n",
    "            for i in range(0,n_clusters):\n",
    "                clustdf = pd.DataFrame(clustered_temporal_footprints[i]).T\n",
    "                stat_key = side+'Resp'+str(i)\n",
    "                stats_dict[stat_key] = complete_stats(clustdf, stat_key)\n",
    "        \n",
    "    return stats_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "def comp_analysis(fill):   \n",
    "    file_stem = str(fill)\n",
    "    \n",
    "    img_speed = 2.73\n",
    "    \n",
    "    sep_type = 3\n",
    "    \n",
    "    if sep_type==1:\n",
    "        #for R/NR\n",
    "        #load rois and get df/f\n",
    "        file_stem_end = '.csv'\n",
    "        left_df = pd.read_csv(file_stem+'closeresp_rln_IpsAllStim'+file_stem_end, index_col=0)\n",
    "        left_nodf = pd.read_csv(file_stem+'closenoresp_rln_IpsAllStim'+file_stem_end, index_col=0)\n",
    "        right_df = pd.read_csv(file_stem+'closeresp_rln_ContAllStim'+file_stem_end, index_col=0)\n",
    "        right_nodf = pd.read_csv(file_stem+'closenoresp_rln_ContAllStim'+file_stem_end, index_col=0)\n",
    "        left_dff = get_dff(left_df)\n",
    "        left_nodff = get_dff(left_nodf)\n",
    "        right_dff = get_dff(right_df)\n",
    "        right_nodff = get_dff(right_nodf)\n",
    "        \n",
    "        #get info for later functions\n",
    "        frames_per_trial = int(left_dff.shape[0])\n",
    "        trial_duration = frames_per_trial/2.73\n",
    "        left_rois = left_df.shape[1]\n",
    "        right_rois = right_df.shape[1]\n",
    "        \n",
    "        #combine sides\n",
    "        tot_dff = pd.concat([left_dff, right_dff], axis=1)\n",
    "        tot_nodff = pd.concat([left_nodff, right_nodff], axis=1)\n",
    "        fijiarr = np.array(tot_dff).T\n",
    "        fiji_noarr = np.array(tot_nodff).T\n",
    "        \n",
    "        n_clusters = fiji_elbow4(fijiarr)\n",
    "        #can use the same batch_labelled_total_list because it is the same rois\n",
    "        batch_labelled_total_list = fiji_kmeans4(n_clusters, fijiarr)\n",
    "        \n",
    "        stats_dict = {}\n",
    "        \n",
    "        for side in ('Ips', 'Cont'):\n",
    "            if side == 'Ips':               \n",
    "                name_for_file = file_stem+side+'Resp'\n",
    "                clustered_temporal_footprints, label_proportions = fiji_organize_roisA(batch_labelled_total_list[0:left_rois],  fijiarr[0:left_rois])\n",
    "\n",
    "                plot_frames = frames_per_trial\n",
    "                plot_duration = plot_frames/img_speed\n",
    "                mean_dict= plot_cluster_avgsONLYA(clustered_temporal_footprints, plot_frames, plot_duration, name_for_file, n_clusters)\n",
    "                fiji_plot_heatmap_clustersA(label_proportions, clustered_temporal_footprints, n_clusters, trial_duration, name_for_file, frames_per_trial)\n",
    "                plot_clust_indandavg(n_clusters, clustered_temporal_footprints, name_for_file)#$%\n",
    "                \n",
    "                for i in range(0,n_clusters):\n",
    "                    clustdf = pd.DataFrame(clustered_temporal_footprints[i]).T\n",
    "                    stat_key = side+'Resp'+str(i)\n",
    "                    stats_dict[stat_key] = complete_stats(clustdf, stat_key)\n",
    "\n",
    "                name_for_file = file_stem+side+'NoResp'\n",
    "                clustered_temporal_footprints, label_proportions = fiji_organize_roisA(batch_labelled_total_list[0:left_rois], fiji_noarr[0:left_rois])\n",
    "                mean_dict = plot_cluster_avgsONLYA(clustered_temporal_footprints, plot_frames, plot_duration, name_for_file, n_clusters)\n",
    "#             mean_dict = plot_cluster_avgsONLY4(clustered_temporal_footprints, plot_frames, plot_duration, name_for_file)\n",
    "                fiji_plot_heatmap_clustersA(label_proportions, clustered_temporal_footprints, n_clusters, trial_duration, name_for_file, frames_per_trial)\n",
    "                plot_clust_indandavg(n_clusters, clustered_temporal_footprints, name_for_file)\n",
    "                for i in range(0,n_clusters):\n",
    "                    clustdf = pd.DataFrame(clustered_temporal_footprints[i]).T\n",
    "                    stat_key = side+'NoResp'+str(i)\n",
    "                    stats_dict[stat_key] = complete_stats(clustdf, stat_key)\n",
    "        \n",
    "#                 tot_left_dff = concat_dff4(trial_list, left_df)\n",
    "#                 clustered_tot_footprints = plot_concated_clusters(batch_labelled_total_list[0:left_rois], tot_left_dff, file_stem, side, n_clusters)\n",
    "\n",
    "            elif side == 'Cont':\n",
    "                name_for_file = file_stem+side+'Resp'\n",
    "                clustered_temporal_footprints, label_proportions = fiji_organize_roisA(batch_labelled_total_list[left_rois:],  fijiarr[left_rois:])\n",
    "\n",
    "                plot_frames = frames_per_trial\n",
    "                plot_duration = plot_frames/img_speed\n",
    "                mean_dict= plot_cluster_avgsONLYA(clustered_temporal_footprints, plot_frames, plot_duration, name_for_file, n_clusters)\n",
    "                fiji_plot_heatmap_clustersA(label_proportions, clustered_temporal_footprints, n_clusters, trial_duration, name_for_file, frames_per_trial)\n",
    "                plot_clust_indandavg(n_clusters, clustered_temporal_footprints, name_for_file)\n",
    "                for i in range(0,n_clusters):\n",
    "                    clustdf = pd.DataFrame(clustered_temporal_footprints[i]).T\n",
    "                    stat_key = side+'Resp'+str(i)\n",
    "                    stats_dict[stat_key] = complete_stats(clustdf, stat_key)\n",
    "\n",
    "                name_for_file = file_stem+side+'NoResp'\n",
    "                clustered_temporal_footprints, label_proportions = fiji_organize_roisA(batch_labelled_total_list[left_rois:], fiji_noarr[left_rois:])\n",
    "                mean_dict = plot_cluster_avgsONLYA(clustered_temporal_footprints, plot_frames, plot_duration, name_for_file, n_clusters)\n",
    "#             mean_dict = plot_cluster_avgsONLY4(clustered_temporal_footprints, plot_frames, plot_duration, name_for_file)\n",
    "                fiji_plot_heatmap_clustersA(label_proportions, clustered_temporal_footprints, n_clusters, trial_duration, name_for_file, frames_per_trial)\n",
    "                plot_clust_indandavg(n_clusters, clustered_temporal_footprints, name_for_file)\n",
    "                for i in range(0,n_clusters):\n",
    "                    clustdf = pd.DataFrame(clustered_temporal_footprints[i]).T\n",
    "                    stat_key = side+'NoResp'+str(i)\n",
    "                    stats_dict[stat_key] = complete_stats(clustdf, stat_key)\n",
    "                    \n",
    "        name_for_file = file_stem+'combined'+'Resp'\n",
    "        plot_combined_sides(batch_labelled_total_list, fijiarr, frames_per_trial, img_speed, name_for_file, n_clusters, trial_duration)\n",
    "        name_for_file = file_stem+'combined'+'noResp'\n",
    "        plot_combined_sides(batch_labelled_total_list, fiji_noarr, frames_per_trial, img_speed, name_for_file, n_clusters, trial_duration)\n",
    "        return 1,batch_labelled_total_list\n",
    "                \n",
    "    if sep_type==2:\n",
    "        file_stem_end = '.csv'\n",
    "        left_df = pd.read_csv(file_stem+'PAG6stim_rln_IpsAllStim'+file_stem_end, index_col=0)\n",
    "        right_df = pd.read_csv(file_stem+'PAG6stim_rln_ContAllStim'+file_stem_end, index_col=0)\n",
    "        \n",
    "        left_dff = get_dff(left_df)\n",
    "        right_dff = get_dff(right_df)\n",
    "        \n",
    "        #get info for later functions\n",
    "        frames_per_trial = int(left_dff.shape[0])\n",
    "        trial_duration = frames_per_trial/2.73\n",
    "        plot_frames = frames_per_trial\n",
    "        plot_duration = plot_frames/img_speed\n",
    "        left_rois = left_df.shape[1]\n",
    "        right_rois = right_df.shape[1]\n",
    "        \n",
    "        tot_dff = pd.concat([left_dff, right_dff], axis=1)\n",
    "        fijiarr = np.array(tot_dff).T\n",
    "        n_clusters = fiji_elbow4(fijiarr)\n",
    "        print('n_cluster right after fiji_elbow is', n_clusters)\n",
    "        batch_labelled_total_list = fiji_kmeans4(n_clusters, fijiarr)\n",
    "        \n",
    "        name_for_file = file_stem+'total'+'Resp'\n",
    "        plot_combined_sides(batch_labelled_total_list, fijiarr, frames_per_trial, img_speed, name_for_file, n_clusters, trial_duration)\n",
    "        return 1,batch_labelled_total_list\n",
    "        \n",
    "    if sep_type==3:\n",
    "        file_stem_end = '.csv'\n",
    "#         Final_sens&beh_PAG6_rln_IpsAllStim\n",
    "#         Final_sens&beh_PAG6_rln_IpsAllStim\n",
    "        left_df = pd.read_csv(file_stem+'PAG6BehSensNov_rlnIpsAllStim'+file_stem_end, index_col=0)\n",
    "        right_df = pd.read_csv(file_stem+'PAG6BehSensNov_rlnContAllStim'+file_stem_end, index_col=0)\n",
    "        left_dff = get_dff(left_df)\n",
    "        right_dff = get_dff(right_df)\n",
    "        \n",
    "        print('The left and right dfs are of shape:', left_df.shape, right_df.shape)\n",
    "        print('The left and right dffs are of shape:', left_dff.shape, right_dff.shape)\n",
    "        \n",
    "        #get info for later functions\n",
    "        frames_per_trial = int(left_dff.shape[0])\n",
    "        trial_duration = frames_per_trial/2.73\n",
    "        plot_frames = frames_per_trial\n",
    "        plot_duration = plot_frames/img_speed\n",
    "        left_rois = left_df.shape[1]\n",
    "        print('left_rois are', left_rois)\n",
    "        right_rois = right_df.shape[1]\n",
    "        \n",
    "        tot_dff = pd.concat([left_dff, right_dff], axis=1)\n",
    "        fijiarr = np.array(tot_dff).T\n",
    "        n_clusters = fiji_elbow4(fijiarr)\n",
    "        print('n_cluster right after fiji_elbow is', n_clusters)\n",
    "        batch_labelled_total_list, kmeans = fiji_kmeansA(n_clusters, fijiarr)\n",
    "        \n",
    "#         stats_dict=1\n",
    "#         return stats_dict , batch_labelled_total_list\n",
    "        \n",
    "        print('The left and right dffs are of shape:', left_dff.shape, right_dff.shape)\n",
    "        print('tot_dff and fijiarr are of shape:', tot_dff.shape, fijiarr.shape)\n",
    "        print('batch_labelled_total_list len is', len(batch_labelled_total_list))\n",
    "        \n",
    "        name_for_file = file_stem+'total'+'Resp'\n",
    "        plot_combined_sides(batch_labelled_total_list, fijiarr, frames_per_trial, img_speed, name_for_file, n_clusters, trial_duration)\n",
    "\n",
    "        \n",
    "        name_for_file = file_stem+'TotIps'\n",
    "        clustered_temporal_footprints, label_proportions = fiji_organize_roisA(batch_labelled_total_list[:left_rois],  fijiarr[:left_rois])\n",
    "        mean_dict= plot_cluster_avgsONLYA(clustered_temporal_footprints, plot_frames, plot_duration, name_for_file, n_clusters)\n",
    "        fiji_plot_heatmap_clustersA(label_proportions, clustered_temporal_footprints, n_clusters, trial_duration, name_for_file, frames_per_trial)\n",
    "#         plot_clust_indandavg(n_clusters, clustered_temporal_footprints, name_for_file)\n",
    "#         for i in range(0,n_clusters):\n",
    "#             clustdf = pd.DataFrame(clustered_temporal_footprints[i]).T\n",
    "#             stat_key = side+'Resp'+str(i)\n",
    "#             stats_dict[stat_key] = complete_stats(clustdf, stat_key)\n",
    "\n",
    "        name_for_file = file_stem+'TotCont'\n",
    "        clustered_temporal_footprints, label_proportions = fiji_organize_roisA(batch_labelled_total_list[left_rois:], fijiarr[left_rois:])\n",
    "        mean_dict = plot_cluster_avgsONLYA(clustered_temporal_footprints, plot_frames, plot_duration, name_for_file, n_clusters)\n",
    "        fiji_plot_heatmap_clustersA(label_proportions, clustered_temporal_footprints, n_clusters, trial_duration, name_for_file, frames_per_trial)\n",
    "#         plot_clust_indandavg(n_clusters, clustered_temporal_footprints, name_for_file)\n",
    "#         for i in range(0,n_clusters):\n",
    "#             clustdf = pd.DataFrame(clustered_temporal_footprints[i]).T\n",
    "#             stat_key = side+'NoResp'+str(i)\n",
    "#             stats_dict[stat_key] = complete_stats(clustdf, stat_key)\n",
    "#         stats_dict=1\n",
    "#         return stats_dict , batch_labelled_total_list\n",
    "                    \n",
    "        types = ['2025', '2535']\n",
    "#         exps = ['rln', 'penk']\n",
    "        all_stats = {}#$%^\n",
    "      \n",
    "        all_stats['combined']=combined_stats(batch_labelled_total_list, left_rois, fijiarr, n_clusters)\n",
    "        print()\n",
    "        for group in types: \n",
    "            stain = 'rln'\n",
    "            group_stats =apply_clust(file_stem, group, stain, kmeans, n_clusters, img_speed, left_rois, right_rois, batch_labelled_total_list)\n",
    "            all_stats[group] = group_stats\n",
    "            print()\n",
    "        return all_stats,batch_labelled_total_list \n",
    "    \n",
    "stats_dict, batch_labelled_total_list = comp_analysis('Final_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renum1 = renum(batch_labelled_total_list, [1,2,0])\n",
    "# renum1 = renum(batch_labelled_total_list, [2,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hold0,hold1 = recolour('Final_', renum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def renum(batch_total_list, act_seq):\n",
    "    letlist = []\n",
    "    cor_list = []\n",
    "    for i in batch_total_list:\n",
    "        if i == act_seq[0]:\n",
    "            letlist.append('a')\n",
    "        elif i == act_seq[1]:\n",
    "            letlist.append('b')\n",
    "        elif i== act_seq[2]:\n",
    "            letlist.append('c')\n",
    "    for i in letlist:\n",
    "        if i =='a':\n",
    "            cor_list.append(0)\n",
    "        elif i=='b':\n",
    "            cor_list.append(1)\n",
    "        elif i=='c':\n",
    "            cor_list.append(2)\n",
    "        else:\n",
    "            print('error')\n",
    "    return cor_list\n",
    "        \n",
    "def recolour(fill, batch_rois):   \n",
    "    file_stem = str(fill)\n",
    "    batch_labelled_total_list = batch_rois\n",
    "    \n",
    "    img_speed = 2.73\n",
    "    \n",
    "    sep_type = 3\n",
    "    \n",
    "    if sep_type==1:\n",
    "        #for R/NR\n",
    "        #load rois and get df/f\n",
    "        file_stem_end = '.csv'\n",
    "        left_df = pd.read_csv(file_stem+'closeresp_rln_IpsAllStim'+file_stem_end, index_col=0)\n",
    "        left_nodf = pd.read_csv(file_stem+'closenoresp_rln_IpsAllStim'+file_stem_end, index_col=0)\n",
    "        right_df = pd.read_csv(file_stem+'closeresp_rln_ContAllStim'+file_stem_end, index_col=0)\n",
    "        right_nodf = pd.read_csv(file_stem+'closenoresp_rln_ContAllStim'+file_stem_end, index_col=0)\n",
    "        left_dff = get_dff(left_df)\n",
    "        left_nodff = get_dff(left_nodf)\n",
    "        right_dff = get_dff(right_df)\n",
    "        right_nodff = get_dff(right_nodf)\n",
    "        \n",
    "        #get info for later functions\n",
    "        frames_per_trial = int(left_dff.shape[0])\n",
    "        trial_duration = frames_per_trial/2.73\n",
    "        left_rois = left_df.shape[1]\n",
    "        right_rois = right_df.shape[1]\n",
    "        \n",
    "        #combine sides\n",
    "        tot_dff = pd.concat([left_dff, right_dff], axis=1)\n",
    "        tot_nodff = pd.concat([left_nodff, right_nodff], axis=1)\n",
    "        fijiarr = np.array(tot_dff).T\n",
    "        fiji_noarr = np.array(tot_nodff).T\n",
    "        \n",
    "        n_clusters = fiji_elbow4(fijiarr)\n",
    "\n",
    "        stats_dict = {}\n",
    "        \n",
    "        for side in ('Ips', 'Cont'):\n",
    "            if side == 'Ips':               \n",
    "                name_for_file = file_stem+side+'Resp'\n",
    "                clustered_temporal_footprints, label_proportions = fiji_organize_roisA(batch_labelled_total_list[0:left_rois],  fijiarr[0:left_rois])\n",
    "\n",
    "                plot_frames = frames_per_trial\n",
    "                plot_duration = plot_frames/img_speed\n",
    "                mean_dict= plot_cluster_avgsONLYA(clustered_temporal_footprints, plot_frames, plot_duration, name_for_file, n_clusters)\n",
    "                fiji_plot_heatmap_clustersA(label_proportions, clustered_temporal_footprints, n_clusters, trial_duration, name_for_file, frames_per_trial)\n",
    "                plot_clust_indandavg(n_clusters, clustered_temporal_footprints, name_for_file)\n",
    "                for i in range(0,n_clusters):\n",
    "                    clustdf = pd.DataFrame(clustered_temporal_footprints[i]).T\n",
    "                    stat_key = side+'Resp'+str(i)\n",
    "                    stats_dict[stat_key] = complete_stats(clustdf, stat_key)\n",
    "\n",
    "                name_for_file = file_stem+side+'NoResp'\n",
    "                clustered_temporal_footprints, label_proportions = fiji_organize_roisA(batch_labelled_total_list[0:left_rois], fiji_noarr[0:left_rois])\n",
    "                mean_dict = plot_cluster_avgsONLYA(clustered_temporal_footprints, plot_frames, plot_duration, name_for_file, n_clusters)\n",
    "#             mean_dict = plot_cluster_avgsONLY4(clustered_temporal_footprints, plot_frames, plot_duration, name_for_file)\n",
    "                fiji_plot_heatmap_clustersA(label_proportions, clustered_temporal_footprints, n_clusters, trial_duration, name_for_file, frames_per_trial)\n",
    "                plot_clust_indandavg(n_clusters, clustered_temporal_footprints, name_for_file)\n",
    "                for i in range(0,n_clusters):\n",
    "                    clustdf = pd.DataFrame(clustered_temporal_footprints[i]).T\n",
    "                    stat_key = side+'NoResp'+str(i)\n",
    "                    stats_dict[stat_key] = complete_stats(clustdf, stat_key)\n",
    "\n",
    "            elif side == 'Cont':\n",
    "                name_for_file = file_stem+side+'Resp'\n",
    "                clustered_temporal_footprints, label_proportions = fiji_organize_roisA(batch_labelled_total_list[left_rois:],  fijiarr[left_rois:])\n",
    "\n",
    "                plot_frames = frames_per_trial\n",
    "                plot_duration = plot_frames/img_speed\n",
    "                mean_dict= plot_cluster_avgsONLYA(clustered_temporal_footprints, plot_frames, plot_duration, name_for_file, n_clusters)\n",
    "                fiji_plot_heatmap_clustersA(label_proportions, clustered_temporal_footprints, n_clusters, trial_duration, name_for_file, frames_per_trial)\n",
    "                plot_clust_indandavg(n_clusters, clustered_temporal_footprints, name_for_file)\n",
    "                for i in range(0,n_clusters):\n",
    "                    clustdf = pd.DataFrame(clustered_temporal_footprints[i]).T\n",
    "                    stat_key = side+'Resp'+str(i)\n",
    "                    stats_dict[stat_key] = complete_stats(clustdf, stat_key)\n",
    "\n",
    "                name_for_file = file_stem+side+'NoResp'\n",
    "                clustered_temporal_footprints, label_proportions = fiji_organize_roisA(batch_labelled_total_list[left_rois:], fiji_noarr[left_rois:])\n",
    "                mean_dict = plot_cluster_avgsONLYA(clustered_temporal_footprints, plot_frames, plot_duration, name_for_file, n_clusters)\n",
    "#             mean_dict = plot_cluster_avgsONLY4(clustered_temporal_footprints, plot_frames, plot_duration, name_for_file)\n",
    "                fiji_plot_heatmap_clustersA(label_proportions, clustered_temporal_footprints, n_clusters, trial_duration, name_for_file, frames_per_trial)\n",
    "                plot_clust_indandavg(n_clusters, clustered_temporal_footprints, name_for_file)\n",
    "                for i in range(0,n_clusters):\n",
    "                    clustdf = pd.DataFrame(clustered_temporal_footprints[i]).T\n",
    "                    stat_key = side+'NoResp'+str(i)\n",
    "                    stats_dict[stat_key] = complete_stats(clustdf, stat_key)\n",
    "                    \n",
    "        name_for_file = file_stem+'combined'+'Resp'\n",
    "        plot_combined_sides(batch_labelled_total_list, fijiarr, frames_per_trial, img_speed, name_for_file, n_clusters, trial_duration)\n",
    "        name_for_file = file_stem+'combined'+'noResp'\n",
    "        plot_combined_sides(batch_labelled_total_list, fiji_noarr, frames_per_trial, img_speed, name_for_file, n_clusters, trial_duration)\n",
    "        return 1,batch_labelled_total_list\n",
    "\n",
    "    if sep_type==2:\n",
    "        file_stem_end = '.csv'\n",
    "        left_df = pd.read_csv(file_stem+'OMRstim_rln_IpsAllStim'+file_stem_end, index_col=0)\n",
    "        right_df = pd.read_csv(file_stem+'OMRstim_rln_ContAllStim'+file_stem_end, index_col=0)\n",
    "        left_dff = get_dff(left_df)\n",
    "        right_dff = get_dff(right_df)\n",
    "        \n",
    "        #get info for later functions\n",
    "        frames_per_trial = int(left_dff.shape[0])\n",
    "        trial_duration = frames_per_trial/2.73\n",
    "        plot_frames = frames_per_trial\n",
    "        plot_duration = plot_frames/img_speed\n",
    "        left_rois = left_df.shape[1]\n",
    "        right_rois = right_df.shape[1]\n",
    "        \n",
    "        tot_dff = pd.concat([left_dff, right_dff], axis=1)\n",
    "        fijiarr = np.array(tot_dff).T\n",
    "        n_clusters = fiji_elbow4(fijiarr)\n",
    "        print('n_cluster right after fiji_elbow is', n_clusters)\n",
    "#         batch_labelled_total_list = fiji_kmeans4(n_clusters, fijiarr)\n",
    "        \n",
    "        name_for_file = file_stem+'total'+'Resp'\n",
    "        plot_combined_sides(batch_labelled_total_list, fijiarr, frames_per_trial, img_speed, name_for_file, n_clusters, trial_duration)\n",
    "        return 1,batch_labelled_total_list\n",
    "        \n",
    "    if sep_type==3:\n",
    "        file_stem_end = '.csv'\n",
    "        left_df = pd.read_csv(file_stem+'PAG6BehSensNov_rlnIpsAllStim'+file_stem_end, index_col=0)\n",
    "        right_df = pd.read_csv(file_stem+'PAG6BehSensNov_rlnContAllStim'+file_stem_end, index_col=0)\n",
    "        left_dff = get_dff(left_df)\n",
    "        right_dff = get_dff(right_df)\n",
    "        \n",
    "        print('The left and right dfs are of shape:', left_df.shape, right_df.shape)\n",
    "        print('The left and right dffs are of shape:', left_dff.shape, right_dff.shape)\n",
    "        \n",
    "        #get info for later functions\n",
    "        frames_per_trial = int(left_dff.shape[0])\n",
    "        trial_duration = frames_per_trial/2.73\n",
    "        plot_frames = frames_per_trial\n",
    "        plot_duration = plot_frames/img_speed\n",
    "        left_rois = left_df.shape[1]\n",
    "        print('left_rois are', left_rois)\n",
    "        right_rois = right_df.shape[1]\n",
    "        \n",
    "        tot_dff = pd.concat([left_dff, right_dff], axis=1)\n",
    "        fijiarr = np.array(tot_dff).T\n",
    "        n_clusters = fiji_elbow4(fijiarr)\n",
    "        print('n_cluster right after fiji_elbow is', n_clusters)\n",
    "        batch_labelled_total_list2, kmeans = fiji_kmeansA(n_clusters, fijiarr)\n",
    "        print('The left and right dffs are of shape:', left_dff.shape, right_dff.shape)\n",
    "        print('tot_dff and fijiarr are of shape:', tot_dff.shape, fijiarr.shape)\n",
    "        print('batch_labelled_total_list len is', len(batch_labelled_total_list))\n",
    "        \n",
    "        name_for_file = file_stem+'total'+'Resp'\n",
    "        plot_combined_sides(batch_labelled_total_list, fijiarr, frames_per_trial, img_speed, name_for_file, n_clusters, trial_duration)\n",
    "\n",
    "        name_for_file = file_stem+'TotIps'\n",
    "        clustered_temporal_footprints, label_proportions = fiji_organize_roisA(batch_labelled_total_list[:left_rois],  fijiarr[:left_rois])\n",
    "        mean_dict= plot_cluster_avgsONLYA(clustered_temporal_footprints, plot_frames, plot_duration, name_for_file, n_clusters)\n",
    "        fiji_plot_heatmap_clustersA(label_proportions, clustered_temporal_footprints, n_clusters, trial_duration, name_for_file, frames_per_trial)\n",
    "#         plot_clust_indandavg(n_clusters, clustered_temporal_footprints, name_for_file)\n",
    "#         for i in range(0,n_clusters):\n",
    "#             clustdf = pd.DataFrame(clustered_temporal_footprints[i]).T\n",
    "#             stat_key = side+'Resp'+str(i)\n",
    "#             stats_dict[stat_key] = complete_stats(clustdf, stat_key)\n",
    "\n",
    "        name_for_file = file_stem+'TotCont'\n",
    "        clustered_temporal_footprints, label_proportions = fiji_organize_roisA(batch_labelled_total_list[left_rois:], fijiarr[left_rois:])\n",
    "        mean_dict = plot_cluster_avgsONLYA(clustered_temporal_footprints, plot_frames, plot_duration, name_for_file, n_clusters)\n",
    "        fiji_plot_heatmap_clustersA(label_proportions, clustered_temporal_footprints, n_clusters, trial_duration, name_for_file, frames_per_trial)\n",
    "#         plot_clust_indandavg(n_clusters, clustered_temporal_footprints, name_for_file)\n",
    "#         for i in range(0,n_clusters):\n",
    "#             clustdf = pd.DataFrame(clustered_temporal_footprints[i]).T\n",
    "#             stat_key = side+'NoResp'+str(i)\n",
    "#             stats_dict[stat_key] = complete_stats(clustdf, stat_key)\n",
    "        \n",
    "        \n",
    "        types = ['2535', '2025']\n",
    "#         exps = ['rln', 'penk']\n",
    "        all_stats = {}\n",
    "    \n",
    "        all_stats['combined']=combined_stats(batch_labelled_total_list, left_rois, fijiarr, n_clusters)\n",
    "        for group in types: \n",
    "            stain = 'rln'\n",
    "            group_stats =apply_clust(file_stem, group, stain, kmeans, n_clusters, img_speed, left_rois, right_rois, batch_labelled_total_list)\n",
    "            all_stats[group] = group_stats\n",
    "        return all_stats,batch_labelled_total_list \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creates bar graph of counts of rois "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roi_count_plots(roidf):\n",
    "    #plots counts of neurons used in the pag6\n",
    "    key_val=0\n",
    "    colourlist = ['deepskyblue', 'orchid', 'white']\n",
    "    tick_place=[]\n",
    "    tick_names = roidf.columns\n",
    "    fig, ax = plt.subplots()\n",
    "    for i in roidf.columns:\n",
    "        if 'rln' in i:\n",
    "            c = colourlist[0]\n",
    "        elif 'penk' in i:\n",
    "            c = colourlist[1]\n",
    "        else:\n",
    "            c = colourlist[2]\n",
    "        rand_val = key_val+(np.random.rand(len(roidf[i]))/4)\n",
    "        plt.scatter(rand_val, roidf[i], alpha=0.7, color=c, s=10)\n",
    "        plt.bar(rand_val.mean(), roidf[i].mean(), width=0.7, alpha=0.4,color=c, yerr=roidf[i].std(), ecolor='dimgrey')\n",
    "        tick_place.append(rand_val.mean())\n",
    "        key_val+=1\n",
    "    ax.set_xticks(tick_place)\n",
    "    ax.set_xticklabels(tick_names)\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "    plt.savefig('GlutGad_coexpproportion_neurons.png')\n",
    "\n",
    "roidf = pd.read_csv('GlutGad_coexpproportion_neurons.csv', index_col=0)\n",
    "roidf.columns\n",
    "roi_count_plots(roidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### does stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_plots():\n",
    "    maxs = []\n",
    "    xvals = []\n",
    "    key_val = 0\n",
    "    for i in keys:\n",
    "        max_val = stats_dict[i]['maxval']\n",
    "    #     print(len(max_val))\n",
    "        rand_val = key_val+(np.random.rand(len(max_val))/4)\n",
    "        maxs.append(max_val)\n",
    "        xvals.append(rand_val)\n",
    "        key_val+=2\n",
    "    \n",
    "    val_mean = []\n",
    "    val_std = []\n",
    "    val_xs = []\n",
    "    for i in range(len(maxs)):\n",
    "        val_mean.append(maxs[i].mean())\n",
    "        val_std.append(maxs[i].std())\n",
    "        val_xs.append(xvals[i].mean())\n",
    "    plt.clf\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xticks(val_xs)\n",
    "    ax.set_xticklabels(keys)\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "\n",
    "\n",
    "    for i in range(len(maxs)):\n",
    "        ax.scatter(xvals[i],maxs[i], c='darkgrey')\n",
    "    ax.scatter(val_xs, val_mean, marker='_', color='black')\n",
    "    ax.errorbar(val_xs, val_mean, yerr=val_std, color='black', linestyle='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = ['baseline', 'maxvalue', 'maxind', 'truemaxtime', '10%base', '10%max', '90%max', 'risetimemax', '%decay']#      %decay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def org_stats_tot(stats_dict, indgroups, stat_choice, window, groupname):\n",
    "\n",
    "    plotgroup_names = indgroups\n",
    "#     print(type(plotgroup_names))\n",
    "    k_groups = {}\n",
    "  \n",
    "    key_val =0 \n",
    "    group = 0\n",
    "    group_color = ['tomato', 'seagreen', 'deepskyblue']\n",
    "    tick_place=[]\n",
    "    tick_names = plotgroup_names\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    fig.set_dpi(300)\n",
    "    \n",
    "    for i in plotgroup_names:\n",
    "        if i in stats_dict[window].keys():\n",
    "            if 'No' in i and 'No' not in groupname:\n",
    "                c = 'grey'\n",
    "            else:\n",
    "                c = group_color[int(i[-1])]\n",
    "                \n",
    "            if stat_choice =='maxvalue':\n",
    "                plt.ylim(0,1.2)\n",
    "            elif stat_choice =='risetimemax':\n",
    "                plt.ylim(0,10)\n",
    "            elif stat_choice =='%decay':\n",
    "                plt.ylim(-5,105)\n",
    "                \n",
    "                \n",
    "            newval=remove_outliers(stats_dict[window][i][stat_choice])# to remove outliers\n",
    "            rand_val = key_val+(np.random.rand(len(newval))/4)\n",
    "            tick_place.append(rand_val.mean())\n",
    "            plt.scatter(rand_val, newval, alpha=0.7, color=c, s=10)# to remove outliers\n",
    "            plt.bar(rand_val.mean(), newval.mean(), width=0.7, alpha=0.5, color=c)#, yerr=newval.std(), ecolor='black', elinewidth=4)\n",
    "            plt.errorbar(rand_val.mean(), newval.mean(),yerr=newval.std(), ecolor='black')#, elinewidth=4)\n",
    "            group+=1\n",
    "#             key_val+=1\n",
    "\n",
    "        else:\n",
    "#             key_val+=0.5\n",
    "            rand_val = key_val\n",
    "            tick_place.append(rand_val)\n",
    "#             plt.scatter(rand_val, 0.1, c='w')\n",
    "            group+=1\n",
    "        if group ==2:\n",
    "            key_val+=0.5\n",
    "        \n",
    "        key_val+=0.9\n",
    "#         if i not in stats_dict[window].keys():\n",
    "#             key_val-=0.5\n",
    "            \n",
    "    print('tick places are:',tick_place)\n",
    "    print('tick names are:', tick_names)\n",
    "    ax.set_xticks(tick_place)\n",
    "    ax.set_xticklabels(tick_names)\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    penkrln = 'rln'\n",
    "\n",
    "    plt.title(window+'_'+penkrln+'_'+stat_choice, pad=10)\n",
    "    filetitle = window+penkrln+stat_choice+groupname+'graph'\n",
    "#     plt.show()\n",
    "#     exportEmf(savePath, filetitle)\n",
    "    plt.savefig('ipscont'+window+penkrln+stat_choice+groupname+'graph.png')\n",
    "\n",
    "def plot_all_stats():\n",
    "    window = ['combined','2535', '2025']\n",
    "    sider = ['Ips','Cont']\n",
    "    sidenr = ['IpsNo','ContNo']\n",
    "    ks = ['2','1']\n",
    "    tot_groups = {}\n",
    "    \n",
    "#     for i in window:\n",
    "#         for j,k in zip(sider, sidenr):\n",
    "#             group=[]\n",
    "#             for q in ks:\n",
    "#                 group.append(j+'Resp'+q)\n",
    "#                 group.append(k+'Resp'+q)\n",
    "# #                 if len(group)==2:\n",
    "# #                     group.append(' ')\n",
    "#             tot_groups[str(i+j)]=group\n",
    "\n",
    "    #plot ips/cont\n",
    "    for i in window:\n",
    "        group=[]\n",
    "        for q in ks:\n",
    "            for j in sider:\n",
    "                group.append(j+'Resp'+q)\n",
    "#                 if len(group)==2:\n",
    "#                     group.append(' ')\n",
    "            tot_groups[i+j]=group\n",
    "        \n",
    "        group=[]\n",
    "        for q in ks:\n",
    "            for j in sidenr:\n",
    "                group.append(j+'Resp'+q)\n",
    "#                 if len(group)==2:\n",
    "#                     group.append(' ')\n",
    "            tot_groups[i+j]=group\n",
    "                   \n",
    "    stat_choices = ['maxvalue','%decay', 'risetimemax']\n",
    "    mm=1\n",
    "        \n",
    "    for i in tot_groups:\n",
    "        if '2535' in i:\n",
    "            window = '2535'\n",
    "        elif '2025' in i:\n",
    "            window='2025'\n",
    "        elif 'combined' in i:\n",
    "            window='combined'\n",
    "        for stat_choice in stat_choices:\n",
    "#             print(tot_groups[i])#, stat_choice, window, i)\n",
    "            org_stats_tot(stats_dict, tot_groups[i], stat_choice, window, i)\n",
    "            print(mm)\n",
    "#             mm+=1\n",
    "            \n",
    "    return tot_groups\n",
    "\n",
    "stats_dict = hold0\n",
    "tot_groups=plot_all_stats()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def org_dif_wind_stats_tot(stats_dict, indgroups, stat_choice, groupname):\n",
    "\n",
    "    plotgroup_names = indgroups\n",
    "    \n",
    "    k_groups = {}\n",
    "    window_names = ['2025', '2535']\n",
    "  \n",
    "    key_val =0 \n",
    "    group = 0\n",
    "    group_color = ['tomato', 'seagreen', 'deepskyblue']\n",
    "    tick_place=[]\n",
    "    tick_names = ['22', '32', '21', '31']\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    fig.set_dpi(300)\n",
    "    \n",
    "    for i in plotgroup_names:\n",
    "        \n",
    "        for window in window_names:\n",
    "        \n",
    "            if i in stats_dict[window].keys():\n",
    "                c = group_color[int(i[-1])]\n",
    "\n",
    "                if stat_choice =='maxvalue':\n",
    "                    plt.ylim(0,1.2)\n",
    "                elif stat_choice =='risetimemax':\n",
    "                    plt.ylim(0,10)\n",
    "                elif stat_choice =='%decay':\n",
    "                    plt.ylim(-5,100)\n",
    "\n",
    "\n",
    "                newval=remove_outliers(stats_dict[window][i][stat_choice])# to remove outliers\n",
    "                rand_val = key_val+(np.random.rand(len(newval))/4)\n",
    "                tick_place.append(rand_val.mean())\n",
    "                plt.scatter(rand_val, newval, alpha=0.7, color=c, s=10)# to remove outliers\n",
    "                plt.bar(rand_val.mean(), newval.mean(), width=0.7, alpha=0.5, color=c)#, yerr=newval.std(), ecolor='black', elinewidth=4)\n",
    "                plt.errorbar(rand_val.mean(), newval.mean(),yerr=newval.std(), ecolor='black')#, elinewidth=4)\n",
    "                group+=1\n",
    "    #             key_val+=1\n",
    "\n",
    "            else:\n",
    "    #             key_val+=0.5\n",
    "                rand_val = key_val\n",
    "                tick_place.append(rand_val)\n",
    "    #             plt.scatter(rand_val, 0.1, c='w')\n",
    "                group+=1\n",
    "            if group ==2:\n",
    "                key_val+=0.5\n",
    "\n",
    "            key_val+=0.9\n",
    "#         if i not in stats_dict[window].keys():\n",
    "#             key_val-=0.5\n",
    "            \n",
    "    print('tick places are:',tick_place)\n",
    "    print('tick names are:', tick_names)\n",
    "    ax.set_xticks(tick_place)\n",
    "    ax.set_xticklabels(tick_names)\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    penkrln = 'rln'\n",
    "\n",
    "    plt.title(window+'_'+penkrln+'_'+stat_choice, pad=10)\n",
    "    filetitle = window+penkrln+stat_choice+groupname+'graph'\n",
    "#     plt.show()\n",
    "#     exportEmf(savePath, filetitle)\n",
    "    plt.savefig('window'+window+penkrln+stat_choice+groupname+'graph.png')\n",
    "\n",
    "def plot_dif_wind_all_stats():\n",
    "    window = ['2535', '2025']\n",
    "    sides = ['Ips','Cont','IpsNo','ContNo']\n",
    "    # sidenr = ['IpsNo','ContNo']\n",
    "    ks = ['2','1']\n",
    "    tot_groups = {}\n",
    "    \n",
    "    for side in sides:\n",
    "        group=[]\n",
    "        for k in ks:\n",
    "            group.append(str(side)+'Resp'+str(k))\n",
    "        tot_groups[str(side)]=group\n",
    "\n",
    "        \n",
    "    stat_choices = ['maxvalue','%decay', 'risetimemax']\n",
    "\n",
    "    for i in tot_groups.keys():\n",
    "        groupen = []\n",
    "        for j in tot_groups[i]:\n",
    "            groupen.append(j)\n",
    "        for choice in stat_choices:\n",
    "            org_dif_wind_stats_tot(stats_dict, groupen, choice, i)\n",
    "    \n",
    "plot_dif_wind_all_stats()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotgroup_names, tick_place, tick_names=org_stats(stats_dict, 'big', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_stats():\n",
    "    window = ['2535', 'close']\n",
    "    sider = ['Ips','Cont']\n",
    "    sidenr = ['IpsNo','ContNo']\n",
    "    ks = ['1','2']\n",
    "    tot_groups = {}   \n",
    "    #plot r/nr\n",
    "    \n",
    "    for i in window:\n",
    "        for j,k in zip(sider, sidenr):\n",
    "            group=[]\n",
    "            for q in ks:\n",
    "                group.append(j+'Resp'+q)\n",
    "                group.append(k+'Resp'+q)\n",
    "                if len(group)==2:\n",
    "                    group.append(' ')\n",
    "            tot_groups[str(i+j)]=group\n",
    "\n",
    "    #plot ips/cont\n",
    "    for i in window:\n",
    "        group=[]\n",
    "        for q in ks:\n",
    "            for j in sider:\n",
    "                group.append(j+'Resp'+q)\n",
    "                if len(group)==2:\n",
    "                    group.append(' ')\n",
    "            tot_groups[i+j]=group\n",
    "        \n",
    "        group=[]\n",
    "        for q in ks:\n",
    "            for j in sidenr:\n",
    "                group.append(j+'Resp'+q)\n",
    "                if len(group)==2:\n",
    "                    group.append(' ')\n",
    "            tot_groups[i+j]=group\n",
    "        \n",
    "#     stat_choices = ['maxvalue','%decay', 'risetimemax']\n",
    "        \n",
    "#     for i in tot_groups:\n",
    "#         print(i)\n",
    "#         if 'big' in i:\n",
    "#             window = 'big'\n",
    "#         else:\n",
    "#             window='close'\n",
    "# #         for stat_choice in stat_choices:\n",
    "# #             org_stats_tot(stats_dict, tot_groups[i], stat_choice, window, i)          \n",
    "            \n",
    "    return tot_groups\n",
    "tot_groups=plot_all_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nt= {'window':0, 'stat_choice':0, 'ICvNRN':0, 'g1':0, 'g2':0, 'stat':0, 'p':0}\n",
    "window = ['combined','2025', '2535']\n",
    "sider = ['IpsResp','ContResp']\n",
    "sidenr = ['IpsNoResp','ContNoResp']\n",
    "ks = ['1','2']\n",
    "conds = ['ic', 'rnr', 'kn']\n",
    "stat_choices = ['maxvalue','%decay', 'risetimemax']\n",
    "mdict={}\n",
    "m=0\n",
    "p1 = 0\n",
    "p2 = 1\n",
    "for wind in window:\n",
    "    for stat_choice in stat_choices:\n",
    "        #doIC first\n",
    "        for cond in conds:\n",
    "            if cond=='ic':\n",
    "                for k in ks:\n",
    "                    if k == '1':\n",
    "                        newnum='2'\n",
    "                    elif k == '2':\n",
    "                        newnum='1'\n",
    "                    else:\n",
    "                        print('didnt work')\n",
    "                    g1= stats_dict[wind][sider[0]+k][stat_choice]#.dropna()\n",
    "                    g2 = stats_dict[wind][sider[1]+k][stat_choice]#.dropna()\n",
    "                    #NN\n",
    "                    g1name = sider[0]+newnum\n",
    "                    g2name = sider[1]+newnum\n",
    "                    g1num = g1.shape[0]\n",
    "                    g2num=g2.shape[0]\n",
    "                    stat, p = ranksums(g1,g2)\n",
    "                    stat_test = 'rank sum'\n",
    "                    if wind == 'combined':\n",
    "                        wind_name = 'combined'\n",
    "                    elif wind == '2025':\n",
    "                        wind_name = 'pre-exp'\n",
    "                    elif wind == '2535':\n",
    "                        wind_name = 'post-exp'\n",
    "                    mdict[m]=(wind_name, stat_choice, 'Ips/Cont', g1name, g1num, g2name, g2num, stat_test, stat, p)\n",
    "                    m+=1\n",
    "                    \n",
    "                    if wind=='combined':\n",
    "                        continue\n",
    "                    if k == 1:\n",
    "                        newnum=2\n",
    "                    elif k == 2:\n",
    "                        newnum=1\n",
    "                        \n",
    "                    g1= stats_dict[wind][sidenr[0]+k][stat_choice]#.dropna()\n",
    "                    g2 = stats_dict[wind][sidenr[1]+k][stat_choice]#.dropna()\n",
    "                    g1name = sidenr[0]+newnum\n",
    "                    g2name = sidenr[1]+newnum\n",
    "                    g1num = g1.shape[0]\n",
    "                    g2num=g2.shape[0]\n",
    "                    stat, p = ranksums(g1,g2)\n",
    "                    stat_test = 'rank sum'\n",
    "                    if wind == 'combined':\n",
    "                        wind_name = 'combined'\n",
    "                    elif wind == '2025':\n",
    "                        wind_name = 'pre-exp'\n",
    "                    elif wind == '2535':\n",
    "                        wind_name = 'post-exp'\n",
    "                    mdict[m]=(wind_name, stat_choice, 'Ips/Cont', g1name, g1num, g2name, g2num, stat_test, stat, p)\n",
    "                    m+=1\n",
    "                    \n",
    "            elif cond=='rnr':\n",
    "                for k in ks:\n",
    "                    if k == '1':\n",
    "                        newnum='2'\n",
    "                    elif k == '2':\n",
    "                        newnum='1'\n",
    "                    else:\n",
    "                        print('didnt work')\n",
    "                    if wind=='combined':\n",
    "                        continue\n",
    "                    g1= stats_dict[wind][sider[0]+k][stat_choice]\n",
    "                    g2 = stats_dict[wind][sidenr[0]+k][stat_choice]\n",
    "                    g1name = sider[0]+newnum\n",
    "                    g2name = sidenr[0]+newnum\n",
    "                    g1num = g1.shape[0]\n",
    "                    g2num=g2.shape[0]\n",
    "                    stat, p = wilcoxon(g1, g2)\n",
    "                    stat_test = 'signed rank'\n",
    "                    if wind == 'combined':\n",
    "                        wind_name = 'combined'\n",
    "                    elif wind == '2025':\n",
    "                        wind_name = 'pre-exp'\n",
    "                    elif wind == '2535':\n",
    "                        wind_name = 'post-exp'\n",
    "                    mdict[m]=(wind_name, stat_choice, 'R/NR', g1name, g1num, g2name, g2num, stat_test, stat, p)\n",
    "                    m+=1\n",
    "                    \n",
    "                    g1= stats_dict[wind][sider[1]+k][stat_choice]\n",
    "                    g2 = stats_dict[wind][sidenr[1]+k][stat_choice]\n",
    "                    g1name = sider[1]+newnum\n",
    "                    g2name = sidenr[1]+newnum\n",
    "                    g1num = g1.shape[0]\n",
    "                    g2num=g2.shape[0]\n",
    "                    stat, p = wilcoxon(g1, g2)\n",
    "                    stat_test = 'signed rank'\n",
    "                    if wind == 'combined':\n",
    "                        wind_name = 'combined'\n",
    "                    elif wind == '2025':\n",
    "                        wind_name = 'pre-exp'\n",
    "                    elif wind == '2535':\n",
    "                        wind_name = 'post-exp'\n",
    "                    mdict[m]=(wind_name, stat_choice, 'R/NR', g1name, g1num, g2name, g2num, stat_test, stat, p)\n",
    "                    m+=1\n",
    "#             if cond=='kn':\n",
    "#                 for side in sider:\n",
    "# #                     if wind =='combined':\n",
    "# #                         print(side, stat_choice)\n",
    "#                     g1=stats_dict[wind][side+'1'][stat_choice]\n",
    "#                     g2=stats_dict[wind][side+'2'][stat_choice]\n",
    "#                     g1name = side+'1'\n",
    "#                     g2name = side+'2'\n",
    "#                     stat, p = ranksums(g1,g2)\n",
    "#                     mdict[m]=(wind, stat_choice, cond, g1name, g2name, stat, p)\n",
    "#                     m+=1\n",
    "#                 for side in sidenr:\n",
    "#                     if wind=='combined':\n",
    "#                         continue\n",
    "#                     g1=stats_dict[wind][side+'1'][stat_choice]\n",
    "#                     g2=stats_dict[wind][side+'2'][stat_choice]\n",
    "#                     stat, p = ranksums(g1,g2)\n",
    "#                     mdict[m]=(wind, stat_choice, cond, g1name, g2name, stat, p)\n",
    "#                     m+=1\n",
    "\n",
    "print(m)\n",
    "newdf = pd.DataFrame(mdict).T\n",
    "newdf.columns=['window', 'stat_choice', 'cond', 'group1', 'group1 # neurons', 'group2', 'group2 # neurons', 'statistical test', 'statistic', 'p value']\n",
    "newdf.to_csv('22rlnallstatscomp.csv')\n",
    "\n",
    "\n",
    "sides = ['IpsResp', 'ContResp']\n",
    "rnr = ['r', 'nr']\n",
    "windows = ['2025', '2535']\n",
    "ks = ['1', '2']\n",
    "stat_choices = ['maxvalue','%decay', 'risetimemax']\n",
    "mdict={}\n",
    "m=0\n",
    "\n",
    "for choice in stat_choices:\n",
    "    for side in sides:        \n",
    "        for k in ks:\n",
    "            if k == '1':\n",
    "                newnum='2'\n",
    "            elif k == '2':\n",
    "                newnum='1'\n",
    "            else:\n",
    "                print('didnt work')\n",
    "#             newval=remove_outliers(stats_dict[window][i][stat_choice])# to remove outliers\n",
    "#             g1 = remove_outliers(stats_dict[windows[0]][side+k][choice])#2025\n",
    "#             print(g1.shape)\n",
    "#             g2 = remove_outliers(stats_dict[windows[1]][side+k][choice])#2535\n",
    "#             print(g2.shape)\n",
    "            g1 = stats_dict[windows[0]][side+k][choice]\n",
    "            g1num = g1.shape[0]\n",
    "            g2 = stats_dict[windows[1]][side+k][choice]\n",
    "            g2num=g2.shape[0]\n",
    "            stat, p = wilcoxon(g1, g2)\n",
    "            g1name = side+'_pre-exp'+newnum\n",
    "            g2name = side+'_post-exp'+newnum\n",
    "            stat_test = 'signed rank'\n",
    "            mdict[m]=(side, choice, k, g1name, g1num, g2name, g2num, stat_test,  stat, p)\n",
    "            m+=1\n",
    "#             print(side+windows[0]+k+choice, side+windows[1]+k+choice)\n",
    "#     print()\n",
    "\n",
    "newdf = pd.DataFrame(mdict).T\n",
    "newdf.columns=['window', 'stat_choice', 'cond', 'group1', 'group1 # neurons', 'group2', 'group2 # neurons', 'statistical test', 'statistic', 'p value']\n",
    "newdf.to_csv('22rlnwindstatscomp.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
